{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/isoittech/SAMPLE_WEB_pytorch_lstm/blob/master/mk_pytorch02_ipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fI1OH1eB4Tse"
   },
   "source": [
    "# 概要\n",
    "\n",
    "PyTorchを使ってLSTMで文章分類を実装してみた。  \n",
    "前の版ではPytorchでLSTMを使って文章分類（ニュース記事のタイトルのカテゴリ分類）を実装しました。  \n",
    "その際、バッチ化はひとまず置いといて（バッチサイズ=1）で実装していましたが、今回はバッチ化対応した。\n",
    "\n",
    "https://qiita.com/m__k/items/db1a81bb06607d5b0ec5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "tHY1ZReN4Ts_"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import linecache\n",
    "from IPython.display import Image\n",
    "import re\n",
    "\n",
    "try:\n",
    "    try:\n",
    "        import MeCab\n",
    "    except ImportError:\n",
    "        # MeCabをcolabで使えるようにする\n",
    "        !apt install aptitude\n",
    "        !aptitude install mecab libmecab-dev mecab-ipadic-utf8 git make curl xz-utils file -y\n",
    "        !pip install mecab-python3\n",
    "        import MeCab # 再挑戦\n",
    "except ImportError as e:\n",
    "  print('Mecab import error')\n",
    "  print(e)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8etgA98H4TtC"
   },
   "source": [
    "# データ準備とバッチ化\n",
    "LSTMのインプットの形式は前回の記事でも言及したように文章の長さ × バッチサイズ × ベクトル次元数の３次元テンソルでした。\n",
    "\n",
    "実際のデータ（ニュース記事のタイトル）の長さ（厳密には形態素の数）は異なりますが、データをバッチ化してまとめてLSTMに流すために文章の系列の長さを揃える必要があります。\n",
    "\n",
    "系列の長さを揃えるために単語リストに新しく<pad>（単語ID＝0）を追加して短い文章を0パディングします。\n",
    "詳細はソースコードを見たほうがわかりやすいと思うので、いきなり全ソースを載せつつ、前回の記事と比べてバッチ化対応するために変更した部分に言及してきます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9ewBZ_0C6TQn",
    "outputId": "c878300f-4c19-4a72-ff7b-20e8a8aea527"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory text exists.\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists('text'):\n",
    "  print('Directory text exists.')\n",
    "else:\n",
    "  try:\n",
    "    try:\n",
    "      from google.colab import drive\n",
    "      drive.mount(r'/content/drive')\n",
    "      !ln -s /usr/local/etc/mecabrc /etc/mecabrc\n",
    "      %cd '/content/drive/MyDrive/Colab Notebooks/SAMPLES/pytorch_lstm'\n",
    "      %pwd\n",
    "      # %ls\n",
    "      !tar xvf ldcc-20140209.tar.gz\n",
    "      %ls\n",
    "\n",
    "    except Exception as e1:\n",
    "      print('Maybe you not Google Colab.')\n",
    "      print(e1)\n",
    "\n",
    "  except Exception as e2:\n",
    "    print('Error')\n",
    "    print(e2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QcBIJvFD4TtF",
    "outputId": "0c727f90-8de9-4fa3-bed2-259fbe2181e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dokujo-tsushin', 'it-life-hack', 'kaden-channel', 'livedoor-homme', 'movie-enter', 'peachy', 'smax', 'sports-watch', 'topic-news']\n"
     ]
    }
   ],
   "source": [
    "# カテゴリを配列で取得\n",
    "categories = [name for name in os.listdir(\"text\") if os.path.isdir(\"text/\" + name)]\n",
    "print(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "id": "4poT4qAK4TtL",
    "outputId": "aa8de328-0d78-4aec-eb7f-aebef6d1a716"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type: $<class 'pandas.core.frame.DataFrame'>\n",
      "shape: $(0, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [title, category]\n",
       "Index: []"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets = pd.DataFrame(columns=[\"title\", \"category\"])\n",
    "print(f\"type: ${type(datasets)}\")\n",
    "print(f\"shape: ${datasets.shape}\")\n",
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 455
    },
    "id": "K5wAAzs44TtN",
    "outputId": "c23dc5b7-203b-49d6-9d32-5467b54eac18"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type: $<class 'pandas.core.frame.DataFrame'>\n",
      "shape: $(7376, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>友人代表のスピーチ、独女はどうこなしている？\\n</td>\n",
       "      <td>dokujo-tsushin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ネットで断ち切れない元カレとの縁\\n</td>\n",
       "      <td>dokujo-tsushin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>相次ぐ芸能人の“すっぴん”披露　その時、独女の心境は？\\n</td>\n",
       "      <td>dokujo-tsushin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ムダな抵抗！？ 加齢の現実\\n</td>\n",
       "      <td>dokujo-tsushin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>税金を払うのは私たちなんですけど！\\n</td>\n",
       "      <td>dokujo-tsushin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7371</th>\n",
       "      <td>爆笑問題・田中裕二も驚く「ひるおび!」での恵俊彰の“天然”ぶり\\n</td>\n",
       "      <td>topic-news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7372</th>\n",
       "      <td>黒田勇樹のDV騒動 ネット掲示板では冷ややかな声も\\n</td>\n",
       "      <td>topic-news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7373</th>\n",
       "      <td>サムスンのアンドロイド搭載カメラが韓国で話題に\\n</td>\n",
       "      <td>topic-news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7374</th>\n",
       "      <td>米紙も注目したゲーム「竹島争奪戦」\\n</td>\n",
       "      <td>topic-news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7375</th>\n",
       "      <td>ジャンプ連載漫画が終了に、ユーザが新たな提案!?\\n</td>\n",
       "      <td>topic-news</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7376 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   title        category\n",
       "0               友人代表のスピーチ、独女はどうこなしている？\\n  dokujo-tsushin\n",
       "1                     ネットで断ち切れない元カレとの縁\\n  dokujo-tsushin\n",
       "2          相次ぐ芸能人の“すっぴん”披露　その時、独女の心境は？\\n  dokujo-tsushin\n",
       "3                        ムダな抵抗！？ 加齢の現実\\n  dokujo-tsushin\n",
       "4                    税金を払うのは私たちなんですけど！\\n  dokujo-tsushin\n",
       "...                                  ...             ...\n",
       "7371   爆笑問題・田中裕二も驚く「ひるおび!」での恵俊彰の“天然”ぶり\\n      topic-news\n",
       "7372         黒田勇樹のDV騒動 ネット掲示板では冷ややかな声も\\n      topic-news\n",
       "7373           サムスンのアンドロイド搭載カメラが韓国で話題に\\n      topic-news\n",
       "7374                 米紙も注目したゲーム「竹島争奪戦」\\n      topic-news\n",
       "7375          ジャンプ連載漫画が終了に、ユーザが新たな提案!?\\n      topic-news\n",
       "\n",
       "[7376 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for cat in categories:\n",
    "    path = \"text/\" + cat + \"/*.txt\"\n",
    "    files = glob(path)\n",
    "    for text_name in files:\n",
    "        title = linecache.getline(text_name, 3)\n",
    "        s = pd.Series([title, cat], index=datasets.columns)\n",
    "        datasets = datasets.append(s, ignore_index=True)\n",
    "\n",
    "print(f\"type: ${type(datasets)}\")\n",
    "print(f\"shape: ${datasets.shape}\")\n",
    "datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bCykLSb44TtP"
   },
   "source": [
    "## 形態素解析エンジン定義\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 658
    },
    "id": "2J_37FhKsdp3",
    "outputId": "d4584173-fa36-4708-c827-95f511cf1d92"
   },
   "outputs": [],
   "source": [
    "tagger = MeCab.Tagger(\"-Owakati\")\n",
    "\n",
    "def make_wakati(sentence):\n",
    "    sentence = tagger.parse(sentence)\n",
    "    sentence = re.sub(r'[0-9０-９a-zA-Zａ-ｚＡ-Ｚ]+', \" \", sentence)\n",
    "    sentence = re.sub(r'[\\．_－―─！＠＃＄％＾＆\\-‐|\\\\＊\\“（）＿■×+α※÷⇒—●★☆〇◎◆▼◇△□(：〜～＋=)／*&^%$#@!~`){}［］…\\[\\]\\\"\\'\\”\\’:;<>?＜＞〔〕〈〉？、。・,\\./『』【】「」→←○《》≪≫\\n\\u3000]+', \"\", sentence)\n",
    "    wakati = sentence.split(\" \")\n",
    "    wakati = list(filter((\"\").__ne__, wakati))\n",
    "    return wakati"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HRiqp3dE4TtU"
   },
   "source": [
    "## 単語IDの辞書を定義\n",
    "単語IDの辞書に新しく\\<pad\\>を追加"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "PFIZpaKJ4TtV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size :  13277\n"
     ]
    }
   ],
   "source": [
    "word2index = {}\n",
    "# 系列を揃えるためのパディング文字列<pad>を追加\n",
    "# パディング文字列のIDは0とする\n",
    "word2index.update({\"<pad>\":0})\n",
    "\n",
    "for title in datasets[\"title\"]:\n",
    "    wakati = make_wakati(title)\n",
    "    for word in wakati:\n",
    "        if word in word2index: continue\n",
    "        word2index[word] = len(word2index)\n",
    "print(\"vocab size : \", len(word2index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kkf0E5AL4TtW"
   },
   "source": [
    "## 系列の長さを揃えてバッチでまとめる\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "94L9Hwlp4TtX"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dokujo-tsushin': 0,\n",
       " 'it-life-hack': 1,\n",
       " 'kaden-channel': 2,\n",
       " 'livedoor-homme': 3,\n",
       " 'movie-enter': 4,\n",
       " 'peachy': 5,\n",
       " 'smax': 6,\n",
       " 'sports-watch': 7,\n",
       " 'topic-news': 8}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "cat2index = {}\n",
    "for cat in categories:\n",
    "    if cat in cat2index: continue\n",
    "    cat2index[cat] = len(cat2index)\n",
    "    \n",
    "cat2index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "9zQ9g3fx4Ttb"
   },
   "outputs": [],
   "source": [
    "def sentence2index(sentence):\n",
    "    wakati = make_wakati(sentence)\n",
    "    return [word2index[w] for w in wakati]\n",
    "\n",
    "def category2index(cat):\n",
    "    return [cat2index[cat]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "rpA3rR7t4Ttc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11],\n",
       " [12, 13, 14, 15, 16, 17, 18, 3, 19],\n",
       " [20, 21, 3, 22, 23, 24, 25, 5, 6, 3, 26, 7],\n",
       " [27, 28, 29, 30, 31, 3, 32],\n",
       " [33, 34, 35, 3, 7, 36, 37, 28, 38, 39, 40],\n",
       " [41, 13, 42, 43, 10, 42, 44, 3, 45, 34, 46, 47, 3, 48],\n",
       " [44, 49, 50, 10, 51, 52, 53, 15, 54, 55, 56, 57],\n",
       " [58, 53, 10, 51, 59, 3, 7, 60, 61, 45, 3, 62],\n",
       " [54, 63, 49, 64, 65, 66, 67, 68, 69, 49, 70, 5, 6, 3, 71, 3, 72, 18, 7],\n",
       " [73, 74, 13, 75, 76, 77, 68, 78, 79, 80, 81]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_datasets_title_tmp = []\n",
    "index_datasets_category = []\n",
    "\n",
    "# 系列の長さの最大値を取得。この長さに他の系列の長さをあわせる\n",
    "max_len = 0\n",
    "for title, category in zip(datasets[\"title\"], datasets[\"category\"]):\n",
    "  index_title = sentence2index(title)\n",
    "  index_category = category2index(category)\n",
    "  index_datasets_title_tmp.append(index_title)\n",
    "  index_datasets_category.append(index_category)\n",
    "  if max_len < len(index_title):\n",
    "    max_len = len(index_title)\n",
    "\n",
    "print(type(index_datasets_title_tmp))\n",
    "index_datasets_title_tmp[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "upHP7rGy4Ttf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "M2cxAQIG4Tth"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[0], [0], [0], [0], [0], [0], [0], [0], [0], [0]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(type(index_datasets_category))\n",
    "index_datasets_category[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "9AGfLUmD4Ttj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  2,\n",
       "  3,\n",
       "  4,\n",
       "  5,\n",
       "  6,\n",
       "  7,\n",
       "  8,\n",
       "  9,\n",
       "  10,\n",
       "  11],\n",
       " [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  12,\n",
       "  13,\n",
       "  14,\n",
       "  15,\n",
       "  16,\n",
       "  17,\n",
       "  18,\n",
       "  3,\n",
       "  19],\n",
       " [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  20,\n",
       "  21,\n",
       "  3,\n",
       "  22,\n",
       "  23,\n",
       "  24,\n",
       "  25,\n",
       "  5,\n",
       "  6,\n",
       "  3,\n",
       "  26,\n",
       "  7]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 系列の長さを揃えるために短い系列にパディングを追加\n",
    "# 後ろパディングだと正しく学習できなかったので、前パディング\n",
    "index_datasets_title = []\n",
    "for title in index_datasets_title_tmp:\n",
    "  for i in range(max_len - len(title)):\n",
    "    title.insert(0, 0) # 前パディング\n",
    "#     title.append(0)　# 後ろパディング\n",
    "  index_datasets_title.append(title)\n",
    "\n",
    "print(type(index_datasets_title))\n",
    "index_datasets_title[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "M2IjpMSj4Tts"
   },
   "outputs": [],
   "source": [
    "train_x, test_x, train_y, test_y = train_test_split(index_datasets_title, index_datasets_category, train_size=0.7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "QaV8juDp4Ttt"
   },
   "outputs": [],
   "source": [
    "# データをバッチでまとめるための関数\n",
    "def train2batch(title, category, batch_size=100):\n",
    "  title_batch = []\n",
    "  category_batch = []\n",
    "  title_shuffle, category_shuffle = shuffle(title, category)\n",
    "  for i in range(0, len(title), batch_size):\n",
    "    title_batch.append(title_shuffle[i:i+batch_size])\n",
    "    category_batch.append(category_shuffle[i:i+batch_size])\n",
    "  return title_batch, category_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KEY64BIX4Ttu"
   },
   "source": [
    "## モデル定義\n",
    "パディング文字列ももちろん埋め込む必要があるわけですが、\\<pad>は0ベクトルで埋め込み、学習の妨げにならないようにする(?)ために、nn.Embedding()にてpadding_idx=0を追加しています。\n",
    "LSTMを定義する際、batch_first=Trueを指定すると、LSTMのインプットの形式がバッチサイズ × 文章の長さ × ベクトル次元数になります。こうしたほうが次元を操作する際にわかりやすいと思います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "jJF_dNb14Ttv"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# GPUを使うために必要\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "sllAMZNn4Ttw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.]]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x = torch.zeros(2, 1, 3)\n",
    "print(test_x.size())\n",
    "test_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "wzcXy0Mr4Ttx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y = torch.squeeze(test_x)\n",
    "print(test_y.size())\n",
    "test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "JTuV9iXU4Ttz"
   },
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        # <pad>の単語IDが0なので、padding_idx=0としている\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        # batch_first=Trueが大事！\n",
    "        # batch_first=Trueを指定すると、LSTMのインプットの形式がバッチサイズ × 文章の長さ(vocab_size) × ベクトル次元数(embedding_dim)になる\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        # LSTMの出力を受け取って全結合してsoftmaxに食わせるための１層のネットワーク\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "        # softmaxのLog版。dim=0で列、dim=1で行方向を確率変換。\n",
    "        self.softmax = nn.LogSoftmax()\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        # embeds.size() => (batch_size × len(sentence) × embedding_dim)\n",
    "        _, lstm_out = self.lstm(embeds)\n",
    "        # lstm_out[0].size() => (1 × batch_size × hidden_dim)\n",
    "        # lstm_outには2個の要素がある。1つ目が最後の隠れ層の隠れ状態h、2つ目が最後の隠れ層のセル状態c。\n",
    "        # many to one パターンであるため、最後の隠れ層の隠れ状態しか使わないので、lstm_out[0]のみを使う（結合層に渡す）。\n",
    "        tag_space = self.hidden2tag(lstm_out[0])\n",
    "        # tag_space.size() => (1 × batch_size × tagset_size)\n",
    "        # (batch_size × tagset_size)にするためにsqueeze()してから（次元を落としてから）、softmax関数にかける。\n",
    "        # https://pytorch.org/docs/stable/generated/torch.squeeze.html\n",
    "        tag_scores = self.softmax(tag_space.squeeze())\n",
    "        # tag_scores.size() = (batch_size × tagset_size)ｈ\n",
    "\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "f4t-q7464Tt7"
   },
   "outputs": [],
   "source": [
    "# 単語の埋め込み次元数上げた。精度がそこそこアップ！ハイパーパラメータのチューニング大事。\n",
    "EMBEDDING_DIM = 200\n",
    "HIDDEN_DIM = 128\n",
    "VOCAB_SIZE = len(word2index)\n",
    "TAG_SIZE = len(categories)\n",
    "# to(device)でモデルがGPU対応する\n",
    "model = LSTMClassifier(EMBEDDING_DIM, HIDDEN_DIM, VOCAB_SIZE, TAG_SIZE).to(device)\n",
    "loss_function = nn.NLLLoss()\n",
    "# SGDからAdamに変更。特に意味はなし\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Be2Ku6xb4Tt9"
   },
   "source": [
    "## 学習\n",
    "1epoch毎に全バッチを学習させます。バッチごとに逆伝搬してパラメータ更新させてます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "2KX5fLV74Tt-"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-19-8fa141d15f80>:26: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  tag_scores = self.softmax(tag_space.squeeze())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 \t loss 93.56144726276398\n",
      "epoch 1 \t loss 62.2303067445755\n",
      "epoch 2 \t loss 43.7643187046051\n",
      "epoch 3 \t loss 28.969568729400635\n",
      "epoch 4 \t loss 18.25108914077282\n",
      "epoch 5 \t loss 10.427849754691124\n",
      "epoch 6 \t loss 5.568141933530569\n",
      "epoch 7 \t loss 2.972314717248082\n",
      "epoch 8 \t loss 1.6831649839878082\n",
      "epoch 9 \t loss 1.037938660942018\n",
      "epoch 10 \t loss 0.7133718775585294\n",
      "epoch 11 \t loss 0.5359514066949487\n",
      "epoch 12 \t loss 0.43793526687659323\n",
      "epoch 13 \t loss 0.36475584493018687\n",
      "epoch 14 \t loss 0.3218360096216202\n",
      "epoch 15 \t loss 0.32037559687159956\n",
      "epoch 16 \t loss 0.2685911732260138\n",
      "epoch 17 \t loss 0.24653033900540322\n",
      "epoch 18 \t loss 0.22653803450521082\n",
      "epoch 19 \t loss 0.21270875295158476\n",
      "epoch 20 \t loss 0.21001775737386197\n",
      "epoch 21 \t loss 0.19516369746997952\n",
      "epoch 22 \t loss 0.18627238902263343\n",
      "epoch 23 \t loss 0.1825403090333566\n",
      "epoch 24 \t loss 0.17467990354634821\n",
      "epoch 25 \t loss 0.17094042181270197\n",
      "epoch 26 \t loss 0.16930701828096062\n",
      "epoch 27 \t loss 0.16362492967164144\n",
      "epoch 28 \t loss 0.15913833386730403\n",
      "epoch 29 \t loss 0.15690695319790393\n",
      "epoch 30 \t loss 0.15287729969713837\n",
      "epoch 31 \t loss 0.15295153626357205\n",
      "epoch 32 \t loss 0.15102554264012724\n",
      "epoch 33 \t loss 0.1444066509429831\n",
      "epoch 34 \t loss 0.1439262943167705\n",
      "epoch 35 \t loss 0.14280017054989003\n",
      "epoch 36 \t loss 0.14104869932634756\n",
      "epoch 37 \t loss 0.13816426586708985\n",
      "epoch 38 \t loss 0.13745228046900593\n",
      "epoch 39 \t loss 0.13878281723009422\n",
      "epoch 40 \t loss 0.13519626745255664\n",
      "epoch 41 \t loss 0.14662554048118182\n",
      "epoch 42 \t loss 0.13497867072874215\n",
      "epoch 43 \t loss 0.1338870490435511\n",
      "epoch 44 \t loss 0.13304387757671066\n",
      "epoch 45 \t loss 0.14276543389132712\n",
      "epoch 46 \t loss 0.1342845984181622\n",
      "epoch 47 \t loss 0.12876495042291936\n",
      "epoch 48 \t loss 0.1286957186239306\n",
      "epoch 49 \t loss 0.12883265711570857\n",
      "epoch 50 \t loss 0.1272096118045738\n",
      "epoch 51 \t loss 0.1274060723517323\n",
      "epoch 52 \t loss 0.1270844694081461\n",
      "epoch 53 \t loss 0.12729474024672527\n",
      "epoch 54 \t loss 0.12327305578219239\n",
      "epoch 55 \t loss 0.12498524162947433\n",
      "epoch 56 \t loss 0.12452849268447608\n",
      "epoch 57 \t loss 0.12296152816998074\n",
      "epoch 58 \t loss 0.12419923979177838\n",
      "epoch 59 \t loss 0.123771081904124\n",
      "epoch 60 \t loss 0.1224264368720469\n",
      "epoch 61 \t loss 0.1220650749237393\n",
      "epoch 62 \t loss 0.12048466576379724\n",
      "epoch 63 \t loss 0.12193000106344698\n",
      "epoch 64 \t loss 0.11911930538917659\n",
      "epoch 65 \t loss 0.12066291114751948\n",
      "epoch 66 \t loss 0.12053145965182921\n",
      "epoch 67 \t loss 0.12008122283077682\n",
      "epoch 68 \t loss 0.11848877153897774\n",
      "epoch 69 \t loss 0.11958622326346813\n",
      "epoch 70 \t loss 0.11729611984628718\n",
      "epoch 71 \t loss 0.11868546204641461\n",
      "epoch 72 \t loss 0.11679150933923665\n",
      "epoch 73 \t loss 0.11807670669440995\n",
      "epoch 74 \t loss 0.1176487972807081\n",
      "epoch 75 \t loss 0.11780221818116843\n",
      "epoch 76 \t loss 0.1164608880681044\n",
      "epoch 77 \t loss 0.11821052420782507\n",
      "epoch 78 \t loss 0.11638725297234487\n",
      "epoch 79 \t loss 0.11653679490700597\n",
      "epoch 80 \t loss 0.11765910585017991\n",
      "epoch 81 \t loss 0.11654913140228018\n",
      "epoch 82 \t loss 0.11739326544193318\n",
      "epoch 83 \t loss 0.11506868616925203\n",
      "epoch 84 \t loss 0.11494400292576756\n",
      "epoch 85 \t loss 0.1158283777476754\n",
      "epoch 86 \t loss 0.11628128809388727\n",
      "epoch 87 \t loss 0.11499271995126037\n",
      "epoch 88 \t loss 0.12592580813179666\n",
      "epoch 89 \t loss 0.11462754031708755\n",
      "epoch 90 \t loss 0.11516365423085517\n",
      "epoch 91 \t loss 0.11462733619919163\n",
      "epoch 92 \t loss 0.12566885147316498\n",
      "epoch 93 \t loss 0.11287849439031561\n",
      "epoch 94 \t loss 0.11442041250302282\n",
      "epoch 95 \t loss 0.11439342939593189\n",
      "epoch 96 \t loss 0.11450292158224329\n",
      "epoch 97 \t loss 0.11414050719395163\n",
      "epoch 98 \t loss 0.11306591898755869\n",
      "epoch 99 \t loss 0.12610635056080355\n",
      "done.\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "for epoch in range(100):\n",
    "    all_loss = 0\n",
    "    title_batch, category_batch = train2batch(train_x, train_y)\n",
    "    for i in range(len(title_batch)):\n",
    "        batch_loss = 0\n",
    "\n",
    "        model.zero_grad()\n",
    "\n",
    "        # 順伝搬させるtensorはGPUで処理させるためdevice=にGPUをセット\n",
    "        title_tensor = torch.tensor(title_batch[i], device=device)\n",
    "        # category_tensor.size() = (batch_size × 1)なので、squeeze()\n",
    "        category_tensor = torch.tensor(category_batch[i], device=device).squeeze()\n",
    "\n",
    "        out = model(title_tensor)\n",
    "\n",
    "        batch_loss = loss_function(out, category_tensor)\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        all_loss += batch_loss.item()\n",
    "    losses.append(all_loss)\n",
    "    print(\"epoch\", epoch, \"\\t\" , \"loss\", all_loss)\n",
    "    if all_loss < 0.1: break\n",
    "print(\"done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "AHKhSv0r4TuA"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f5842f53730>]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAD4CAYAAAD1jb0+AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Z1A+gAAAACXBIWXMAAAsTAAALEwEAmpwYAAAVh0lEQVR4nO3dfYwcd33H8fd3d2/3btc+30POjmPHDyGRYxeJOj2RoFQ0iqENDyJRhaog2lpVKqsSLeFBhSCkov5HJERIJQSyEqiBEGhN2qQRpQJjQKiN4ZxASHyGOA927Jztww+x47Pvab/9Y2bP68udb+9hb/yb+byk8+3Mzux8x3P3ud/+9jcz5u6IiEh4ckkXICIic6MAFxEJlAJcRCRQCnARkUApwEVEAlVYzI1dddVVvm7dusXcpIhI8Pbu3ft7d++ZPH9RA3zdunX09fUt5iZFRIJnZgenmq8uFBGRQCnARUQCpQAXEQmUAlxEJFAKcBGRQCnARUQCpQAXEQlUEAH+H88c5ltPTTkMUkQks4II8Cd/PcCjvziUdBkiIleUIAK8XCowNDKedBkiIleUIAK8Usxzbngs6TJERK4oQQR4uagWuIjIZIEEeJ6hkTF0/04RkYvCCPBSnqrD8Fg16VJERK4YQQR4pRhd9Vb94CIiFwUR4OViHkD94CIidYII8EopboGPqAUuIlITRIDXWuDnhtUCFxGpCSLAay3wIbXARUQmBBHgaoGLiLxZIAEetcDPj6oFLiJSE0SAV9QCFxF5kyACvKw+cBGRNwkiwNta1AIXEZksiADP54y2lrxa4CIidYIIcIBKKc85nYkpIjIhmAAvFwsM6VooIiITAgpwtcBFROoFFeDnFeAiIhOCCfBKqaCLWYmI1AkmwMvFPEMaRigiMiGYAK8U1QIXEakXTICXS3nd0EFEpE4wAV4pFnRLNRGROg0FuJl93MyeN7PnzOxRM2s1s/VmtsfMDpjZd82s2MxCy8UCw2NVxsZ1Y2MREWggwM1sFfBRoNfd3wrkgbuB+4EH3P164BRwTzMLrZTi+2KOqhtFRAQa70IpAG1mVgDKwABwO7Azfn4HcNeCV1enrXZjY41EEREBGghwdz8CfAE4RBTcrwN7gdPuXuuUPgysmmp9M9tmZn1m1jc4ODjnQitFXVJWRKReI10oncCdwHrgGqAC3NHoBtx9u7v3untvT0/PnAut3VZNI1FERCKNdKG8C3jZ3QfdfRR4DLgV6Ii7VABWA0eaVCNw8cbGGokiIhJpJMAPAbeYWdnMDNgC7AN2Ax+Ml9kKPN6cEiNqgYuIXKqRPvA9RB9WPg38Jl5nO/Bp4BNmdgDoBh5uYp0XW+DqAxcRAaLRJTNy988Bn5s0+yXg7Qte0TTKGoUiInKJoM7EBLXARURqggnwNvWBi4hcIpgALxVy5HOmceAiIrFgAtzMotuqqQ9cRAQIKMAh6gdXC1xEJBJUgJdLurGxiEhNUAFeKRYY0pmYIiJAYAFeLqoFLiJSE1SAV0rqAxcRqQkqwNt0Z3oRkQlBBXilqBsbi4jUBBXg5WJBp9KLiMSCCvBKKWqBu3vSpYiIJC6oAC8XC4xXneEx3ZleRCSoAK/oglYiIhOCCvCybqsmIjIhqAC/eGd6tcBFRIIK8NpdeTQSRUQk0AA/rxa4iEhYAV5RH7iIyISgArysUSgiIhOCCvCJFrj6wEVEwgrwiRa4LmglIhJagKsFLiJSE1SA53NGa0tOfeAiIgQW4BBfkVCjUEREQgzwvMaBi4gQYIBXdE1wEREgwAAvl3RXHhERCDDAK+oDFxEBAgzwsu6LKSICBBjglVKBsxfUAhcRCS7AO8otvH5+NOkyREQS11CAm1mHme00s/1m1m9m7zCzLjP7oZm9EH/vbHaxAF3lIm8MjzE8pm4UEcm2RlvgDwI/cPcbgbcB/cB9wC53vwHYFU83XdeSIgCnzqkVLiLZNmOAm9ky4J3AwwDuPuLup4E7gR3xYjuAu5pT4qW6ylGAnzw3shibExG5YjXSAl8PDAJfN7NnzOwhM6sAK9x9IF7mKLBiqpXNbJuZ9ZlZ3+Dg4LwL7qzELfAhBbiIZFsjAV4AbgK+4u6bgXNM6i5xdwd8qpXdfbu797p7b09Pz3zrpauiFriICDQW4IeBw+6+J57eSRTox8xsJUD8/XhzSrxUZ1ktcBERaCDA3f0o8KqZbYhnbQH2AU8AW+N5W4HHm1LhJB3lFkAtcBGRQoPL/QPwiJkVgZeAvyEK/38zs3uAg8BfNKfES7Xkcyxra+GUAlxEMq6hAHf3XwG9Uzy1ZUGraVBXpcjJIQ0jFJFsC+5MTIDOcgsnzw0nXYaISKKCDPCuSpGTOpFHRDIuyADvLBfVBy4imRdkgEd94CNEw89FRLIp2AAfGavquuAikmlBBninzsYUEQkzwLt0NqaISJgBrha4iEigAa4LWomIhBrguia4iEiYAd7eViCfM/WBi0imBRngZkZnWWdjiki2BRngAF0VXZFQRLIt2ADvLEdnY4qIZFWwAd5V0fVQRCTbgg3wzkpRo1BEJNOCDfDuSpFTQyNUq7qglYhkU7AB3lkuUnU4c0EjUUQkm4INcJ2NKSJZF2yA166HopN5RCSrgg3wi6fTqwtFRLIp2ADvrLQAaCihiGRWsAHeXSkB6GQeEcmsYAO8rZintSWnDzFFJLOCDXCI+sEV4CKSVUEHeKdOpxeRDAs6wLsquqCViGRX0AHeWVYLXESyK+gA76oUOfGGAlxEsinoAO9ZWuLs8BgXRseTLkVEZNEFHeDLl0ZjwY+fGU64EhGRxRd2gLe3AnD87IWEKxERWXxhB3itBX5WLXARyZ6GA9zM8mb2jJk9GU+vN7M9ZnbAzL5rZsXmlTm1i10oaoGLSPbMpgV+L9BfN30/8IC7Xw+cAu5ZyMIa0VkuUsgZx9QCF5EMaijAzWw18D7goXjagNuBnfEiO4C7mlDfZeVyRs/Skj7EFJFMarQF/iXgU0A1nu4GTrv7WDx9GFi1sKU1ZvnSkj7EFJFMmjHAzez9wHF33zuXDZjZNjPrM7O+wcHBubzEZS1vb2VQXSgikkGNtMBvBT5gZq8A3yHqOnkQ6DCzQrzMauDIVCu7+3Z373X33p6engUo+VJRC1wBLiLZM2OAu/tn3H21u68D7gZ+7O4fBnYDH4wX2wo83rQqL2P50lZOnhthZKw688IiIikyn3HgnwY+YWYHiPrEH16YkmZneXs0lHDwDbXCRSRbCjMvcpG7/wT4Sfz4JeDtC1/S7NSPBV/V0ZZwNSIiiyfoMzEh6kIBnY0pItkTfoC363R6Ecmm4AO8u1IkZzCo0+lFJGOCD/BCPkf3Eg0lFJHsCT7AQWPBRSSbUhPgx9SFIiIZk5IAb1ULXEQyJx0B3l7ixBvDjFc96VJERBZNOgJ8aYmqwwmdjSkiGZKKAO/RyTwikkGpCPAVEyfz6INMEcmOVAR47e70x3RnHhHJkFQEeM+S2gWtFOAikh2pCPBiIUdnuUVdKCKSKakIcNBYcBHJnvQEeLtOpxeRbElNgPcsLemKhCKSKakJ8BXtURdKVWdjikhGpCbAr+loY6zqHNMHmSKSEakJ8LVdZQAOnRhKuBIRkcWRmgBfEwf4wZMKcBHJhtQE+KrONvI5UwtcRDIjNQHeks9xTUcrh9QCF5GMSE2AA6ztqqgLRUQyI1UBfm1XmUMnziVdhojIokhVgK/tLnNqaJQzF0aTLkVEpOnSFeAaSigiGZKqAF/THQe4+sFFJAPSFeC1seBqgYtIBqQqwJe2ttBVKaoFLiKZkKoAh6gVfuikRqKISPqlLsDXdpfVhSIimZC6AF/TVea10+cZGasmXYqISFOlMsCrDq+dPp90KSIiTTVjgJvZtWa228z2mdnzZnZvPL/LzH5oZi/E3zubX+7M1nZXAF2VUETSr5EW+BjwSXffBNwCfMTMNgH3Abvc/QZgVzyduLW1seA6pV5EUm7GAHf3AXd/On58FugHVgF3AjvixXYAdzWpxlnpWVKiVMhpKKGIpN6s+sDNbB2wGdgDrHD3gfipo8CKadbZZmZ9ZtY3ODg4n1obkssZa7o0EkVE0q/hADezJcD3gI+5+5n659zdgSnvJuzu29291917e3p65lVso9Z2l9UCF5HUayjAzayFKLwfcffH4tnHzGxl/PxK4HhzSpy9NV0VDp0cIvq7IiKSTo2MQjHgYaDf3b9Y99QTwNb48Vbg8YUvb27WdpcZGhnn+NnhpEsREWmaRlrgtwJ/BdxuZr+Kv94LfB54t5m9ALwrnr4ibLh6KQD9A2dmWFJEJFyFmRZw958DNs3TWxa2nIWx8ep2APoHznLbhuUJVyMi0hypOxMTYFm5hVUdbexTC1xEUiyVAQ6wcWW7ulBEJNVSG+CbrmnnpcE3uDA6nnQpIiJNkd4AX7mUqsNvj55NuhQRkaZIcYAvA1A/uIikVmoDfHVnG0tKBfWDi0hqpTbAczlj48ql7HtNAS4i6ZTaAIdoJMr+o2epVnVKvYikT6oDfNPKdt4YHuPVU7qwlYikT6oDfOPK6IxMdaOISBqlOsA3XL2UnOmaKCKSTqkO8NaWPNf1LNFQQhFJpVQHOET94P0DOplHRNIn9QG+cWU7R06f5/TQSNKliIgsqNQH+OY1HQA89dLJZAsREVlgqQ/wP1rbydLWArv6jyVdiojIgkp9gLfkc9y2YTm7f3tcJ/SISKqkPsABtty4nN+/McKvD59OuhQRkQWTiQC/bUMPOYMf7z+edCkiIgsmEwHeUS7Su7aLH/UrwEUkPTIR4ABbNi6nf+AMr50+n3QpIiILIkMBvgKAXepGEZGUyEyAv6WnwtrusoYTikhqZCbAzYwtN67gf188wdDIWNLliIjMW2YCHOBP/2AFI2NVdu49nHQpIiLzlqkAv3l9F++4rpsv/egFzlwYTbocEZF5yVSAmxmffd9GTg2N8OXdB5IuR0RkXjIV4ABvXbWMP9+8mq///BVePalbrYlIuDIX4AD/+GcbyOXg/h/sT7oUEZE5y2SAX72slW3vfAtPPjvA938zkHQ5IiJzkskAB/i7P7mOzWs6+Mi3n+Yb//dK0uWIiMxaZgO8XCzw7b+9hS03ruCfHn+ez//3fl1uVkSCktkAB2gr5vnqX97Eh29ew1d/+iLvfuCnPLLnIOdHxpMuTURkRuY+91anmd0BPAjkgYfc/fOXW763t9f7+vrmvL1mcXf+69kBtv/sRZ47cobOcgu3bVjOTWs62Lymk+uXL6G1JZ90mSKSUWa219173zR/rgFuZnngd8C7gcPAL4EPufu+6da5UgO8xt35xcsn+eZTB9nz8kkGzw5PPHfVkhKrO9u4akmJZW0tdJRbWNpaYEmpQLlYoFzMUyzkKBVy5HOGmQGQN6O1JUdrS37iuUIuRz5vFHJGzox8zsibkctBzox4VSCari2TMyZeV0SyY7oAL8zjNd8OHHD3l+INfAe4E5g2wK90ZsbN13Vz83XduDuHT53n6UOnOHhiiCOnznP49BCHTw2x77VRXj8/yrmEulrMwCYeG1abFz++uIxdsg4QLxs/Yxenp3vtqeTileuXnaneeA1m+vtjgANVd6oO7pCv/WG7TE3R8o57tL3Jy0f/Pw3U2tAeMfGa8zXdSyzkH+p5vdIClVF7mbm+359NGR7/U3XH43UnN4ymWsfjdeqXn89x8Pjnsfaz/K17bmZNd3nOrzeV+QT4KuDVuunDwM2TFzKzbcA2gDVr1sxjc4vLzLi2q8y1XdP/h49XnXMjY5wbHuP8yDgj41WGR6uM1X0YOl51LoyOc2F0nOGxKuNVZ7zqjFWrjFdhvBrP8+iAj9etOxFkVWe8Ck4t1KJl3KN50fdo2dqK9b8otR+k2mvWr8fk5SYeT73PtfWqtd+SGUxsN95mI8sCl7yL8Ticp/uM+ZKQNy75pZnYbgPJMZtwmTgGzBwu0y0z3fYaeVPcyHYnb8Pdpwyky82fjUb3c7q6Z/v/dDnRO9f6BgHRIIVpNjDxRz9+fmL5ecrF75xzZpRaFv4jx/kEeEPcfTuwHaIulGZvbzHlc0Z7awvtrS1JlyIiGTSfPwlHgGvrplfH80REZBHMJ8B/CdxgZuvNrAjcDTyxMGWJiMhM5tyF4u5jZvb3wP8QDSP8mrs/v2CViYjIZc2rD9zdvw98f4FqERGRWcj0mZgiIiFTgIuIBEoBLiISKAW4iEig5nUxq1lvzGwQODjH1a8Cfr+A5YQii/udxX2GbO639rkxa929Z/LMRQ3w+TCzvqku5pJ2WdzvLO4zZHO/tc/zoy4UEZFAKcBFRAIVUoBvT7qAhGRxv7O4z5DN/dY+z0MwfeAiInKpkFrgIiJSRwEuIhKoIALczO4ws9+a2QEzuy/peprBzK41s91mts/Mnjeze+P5XWb2QzN7If7emXStC83M8mb2jJk9GU+vN7M98fH+bny54lQxsw4z22lm+82s38zekfZjbWYfj3+2nzOzR82sNY3H2sy+ZmbHzey5unlTHluL/Eu8/8+a2U2z2dYVH+DxzZO/DLwH2AR8yMw2JVtVU4wBn3T3TcAtwEfi/bwP2OXuNwC74um0uRfor5u+H3jA3a8HTgH3JFJVcz0I/MDdbwTeRrT/qT3WZrYK+CjQ6+5vJboE9d2k81j/K3DHpHnTHdv3ADfEX9uAr8xmQ1d8gFN382R3HwFqN09OFXcfcPen48dniX6hVxHt6454sR3AXYkU2CRmthp4H/BQPG3A7cDOeJE07vMy4J3AwwDuPuLup0n5sSa6fHWbmRWAMjBACo+1u/8MODlp9nTH9k7gGx55Cugws5WNbiuEAJ/q5smrEqplUZjZOmAzsAdY4e4D8VNHgRVJ1dUkXwI+BVTj6W7gtLuPxdNpPN7rgUHg63HX0UNmViHFx9rdjwBfAA4RBffrwF7Sf6xrpju288q3EAI8U8xsCfA94GPufqb+OY/GfKZm3KeZvR847u57k65lkRWAm4CvuPtm4ByTuktSeKw7iVqb64FrgApv7mbIhIU8tiEEeGZunmxmLUTh/Yi7PxbPPlZ7SxV/P55UfU1wK/ABM3uFqGvsdqK+4Y74bTak83gfBg67+554eidRoKf5WL8LeNndB919FHiM6Pin/VjXTHds55VvIQR4Jm6eHPf9Pgz0u/sX6556AtgaP94KPL7YtTWLu3/G3Ve7+zqi4/pjd/8wsBv4YLxYqvYZwN2PAq+a2YZ41hZgHyk+1kRdJ7eYWTn+Wa/tc6qPdZ3pju0TwF/Ho1FuAV6v62qZmbtf8V/Ae4HfAS8Cn026nibt4x8Tva16FvhV/PVeoj7hXcALwI+ArqRrbdL+3wY8GT++DvgFcAD4d6CUdH1N2N8/BPri4/2fQGfajzXwz8B+4Dngm0ApjccaeJSon3+U6N3WPdMdW8CIRtm9CPyGaJROw9vSqfQiIoEKoQtFRESmoAAXEQmUAlxEJFAKcBGRQCnARUQCpQAXEQmUAlxEJFD/DwdBhZLpqoduAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kW1zXDsK4TuB"
   },
   "source": [
    "## 予測\n",
    "バッチ毎にまとめて予測。\n",
    "前回と比べて精度が上がっているのは単語の埋め込み次元数を上げたからだと思われます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "2lGNp8Bb4TuC"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-23-f4958859d2b4>:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  title_tensor = torch.tensor(title_batch[i], device=device)\n",
      "<ipython-input-23-f4958859d2b4>:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  category_tensor = torch.tensor(category_batch[i], device=device)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.FloatTensor instead (while checking arguments for embedding)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-f4958859d2b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mcategory_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcategory_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitle_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mans\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcategory_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/pytorch_lstm-08jE_MvW/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-8fa141d15f80>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, sentence)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0membeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0;31m# embeds.size() => (batch_size × len(sentence) × embedding_dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlstm_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/pytorch_lstm-08jE_MvW/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/pytorch_lstm-08jE_MvW/lib/python3.8/site-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         return F.embedding(\n\u001b[0m\u001b[1;32m    157\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n",
      "\u001b[0;32m~/.local/share/virtualenvs/pytorch_lstm-08jE_MvW/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   1914\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1915\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1916\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1917\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.FloatTensor instead (while checking arguments for embedding)"
     ]
    }
   ],
   "source": [
    "test_num = len(test_x)\n",
    "a = 0\n",
    "with torch.no_grad():\n",
    "    title_batch, category_batch = train2batch(test_x, test_y)\n",
    "\n",
    "    for i in range(len(title_batch)):\n",
    "        title_tensor = torch.tensor(title_batch[i], device=device)\n",
    "        category_tensor = torch.tensor(category_batch[i], device=device)\n",
    "\n",
    "        out = model(title_tensor)\n",
    "        _, predicts = torch.max(out, 1)\n",
    "        for j, ans in enumerate(category_tensor):\n",
    "            if predicts[j].item() == ans.item():\n",
    "                a += 1\n",
    "print(\"predict : \", a / test_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9gb8Egc04TuQ"
   },
   "source": [
    "## 最後に\n",
    "バッチ化対応の一番苦労したところはやはり次元の扱いでした。私と同じように次元数によるエラーで躓いている方は、全てのtensorの次元を逐一確認して、データの形状を細かく追ってみるのがよいと思います。"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "name": "mk_pytorch02.ipynb のコピー",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python(pytorch_lstm)",
   "language": "python",
   "name": "pytorch_lstm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
