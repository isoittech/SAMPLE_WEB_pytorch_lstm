{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "toc-showtags": false,
    "colab": {
      "name": "mk_pytorch02.ipynb のコピー",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/isoittech/SAMPLE_WEB_pytorch_lstm/blob/master/mk_pytorch02_ipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fI1OH1eB4Tse"
      },
      "source": [
        "# 概要\n",
        "\n",
        "PyTorchを使ってLSTMで文章分類を実装してみた。  \n",
        "前の版ではPytorchでLSTMを使って文章分類（ニュース記事のタイトルのカテゴリ分類）を実装しました。  \n",
        "その際、バッチ化はひとまず置いといて（バッチサイズ=1）で実装していましたが、今回はバッチ化対応した。\n",
        "\n",
        "https://qiita.com/m__k/items/db1a81bb06607d5b0ec5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHY1ZReN4Ts_"
      },
      "source": [
        "import os\n",
        "from glob import glob\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import linecache\n",
        "from IPython.display import Image\n",
        "import re\n",
        "\n",
        "try:\n",
        "    try:\n",
        "        import MeCab\n",
        "    except ImportError:\n",
        "        # MeCabをcolabで使えるようにする\n",
        "        !apt install aptitude\n",
        "        !aptitude install mecab libmecab-dev mecab-ipadic-utf8 git make curl xz-utils file -y\n",
        "        !pip install mecab-python3\n",
        "        import MeCab # 再挑戦\n",
        "except ImportError as e:\n",
        "  print('Mecab import error')\n",
        "  print(e)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8etgA98H4TtC"
      },
      "source": [
        "# データ準備とバッチ化\n",
        "LSTMのインプットの形式は前回の記事でも言及したように文章の長さ × バッチサイズ × ベクトル次元数の３次元テンソルでした。\n",
        "\n",
        "実際のデータ（ニュース記事のタイトル）の長さ（厳密には形態素の数）は異なりますが、データをバッチ化してまとめてLSTMに流すために文章の系列の長さを揃える必要があります。\n",
        "\n",
        "系列の長さを揃えるために単語リストに新しく<pad>（単語ID＝0）を追加して短い文章を0パディングします。\n",
        "詳細はソースコードを見たほうがわかりやすいと思うので、いきなり全ソースを載せつつ、前回の記事と比べてバッチ化対応するために変更した部分に言及してきます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ewBZ_0C6TQn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c878300f-4c19-4a72-ff7b-20e8a8aea527"
      },
      "source": [
        "if os.path.exists('text'):\n",
        "  print('Directory text exists.')\n",
        "else:\n",
        "  try:\n",
        "    try:\n",
        "      from google.colab import drive\n",
        "      drive.mount(r'/content/drive')\n",
        "      %cd '/content/drive/MyDrive/Colab Notebooks/SAMPLES/pytorch_lstm'\n",
        "      %pwd\n",
        "      # %ls\n",
        "      !tar xvf ldcc-20140209.tar.gz\n",
        "      %ls\n",
        "\n",
        "    except Exception as e1:\n",
        "      print('Maybe you not Google Colab.')\n",
        "      print(e1)\n",
        "\n",
        "  except Exception as e2:\n",
        "    print('Error')\n",
        "    print(e2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/MyDrive/Colab Notebooks/SAMPLES/pytorch_lstm\n",
            "Directory text exists.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QcBIJvFD4TtF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c727f90-8de9-4fa3-bed2-259fbe2181e2"
      },
      "source": [
        "# カテゴリを配列で取得\n",
        "categories = [name for name in os.listdir(\"text\") if os.path.isdir(\"text/\" + name)]\n",
        "print(categories)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['dokujo-tsushin', 'it-life-hack', 'kaden-channel', 'livedoor-homme', 'movie-enter', 'peachy', 'smax', 'sports-watch', 'topic-news']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4poT4qAK4TtL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "aa8de328-0d78-4aec-eb7f-aebef6d1a716"
      },
      "source": [
        "datasets = pd.DataFrame(columns=[\"title\", \"category\"])\n",
        "print(f\"type: ${type(datasets)}\")\n",
        "print(f\"shape: ${datasets.shape}\")\n",
        "datasets"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "type: $<class 'pandas.core.frame.DataFrame'>\n",
            "shape: $(0, 2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [title, category]\n",
              "Index: []"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5wAAzs44TtN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        },
        "outputId": "c23dc5b7-203b-49d6-9d32-5467b54eac18"
      },
      "source": [
        "for cat in categories:\n",
        "    path = \"text/\" + cat + \"/*.txt\"\n",
        "    files = glob(path)\n",
        "    for text_name in files:\n",
        "        title = linecache.getline(text_name, 3)\n",
        "        s = pd.Series([title, cat], index=datasets.columns)\n",
        "        datasets = datasets.append(s, ignore_index=True)\n",
        "\n",
        "print(f\"type: ${type(datasets)}\")\n",
        "print(f\"shape: ${datasets.shape}\")\n",
        "datasets"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "type: $<class 'pandas.core.frame.DataFrame'>\n",
            "shape: $(7376, 2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>友人代表のスピーチ、独女はどうこなしている？\\n</td>\n",
              "      <td>dokujo-tsushin</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ネットで断ち切れない元カレとの縁\\n</td>\n",
              "      <td>dokujo-tsushin</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>相次ぐ芸能人の“すっぴん”披露　その時、独女の心境は？\\n</td>\n",
              "      <td>dokujo-tsushin</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ムダな抵抗！？ 加齢の現実\\n</td>\n",
              "      <td>dokujo-tsushin</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>税金を払うのは私たちなんですけど！\\n</td>\n",
              "      <td>dokujo-tsushin</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7371</th>\n",
              "      <td>爆笑問題・田中裕二も驚く「ひるおび!」での恵俊彰の“天然”ぶり\\n</td>\n",
              "      <td>topic-news</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7372</th>\n",
              "      <td>黒田勇樹のDV騒動 ネット掲示板では冷ややかな声も\\n</td>\n",
              "      <td>topic-news</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7373</th>\n",
              "      <td>サムスンのアンドロイド搭載カメラが韓国で話題に\\n</td>\n",
              "      <td>topic-news</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7374</th>\n",
              "      <td>米紙も注目したゲーム「竹島争奪戦」\\n</td>\n",
              "      <td>topic-news</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7375</th>\n",
              "      <td>ジャンプ連載漫画が終了に、ユーザが新たな提案!?\\n</td>\n",
              "      <td>topic-news</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>7376 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                   title        category\n",
              "0               友人代表のスピーチ、独女はどうこなしている？\\n  dokujo-tsushin\n",
              "1                     ネットで断ち切れない元カレとの縁\\n  dokujo-tsushin\n",
              "2          相次ぐ芸能人の“すっぴん”披露　その時、独女の心境は？\\n  dokujo-tsushin\n",
              "3                        ムダな抵抗！？ 加齢の現実\\n  dokujo-tsushin\n",
              "4                    税金を払うのは私たちなんですけど！\\n  dokujo-tsushin\n",
              "...                                  ...             ...\n",
              "7371   爆笑問題・田中裕二も驚く「ひるおび!」での恵俊彰の“天然”ぶり\\n      topic-news\n",
              "7372         黒田勇樹のDV騒動 ネット掲示板では冷ややかな声も\\n      topic-news\n",
              "7373           サムスンのアンドロイド搭載カメラが韓国で話題に\\n      topic-news\n",
              "7374                 米紙も注目したゲーム「竹島争奪戦」\\n      topic-news\n",
              "7375          ジャンプ連載漫画が終了に、ユーザが新たな提案!?\\n      topic-news\n",
              "\n",
              "[7376 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCykLSb44TtP"
      },
      "source": [
        "## 形態素解析エンジン定義\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2J_37FhKsdp3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 658
        },
        "outputId": "d4584173-fa36-4708-c827-95f511cf1d92"
      },
      "source": [
        "tagger = MeCab.Tagger(\"-Owakati\")\n",
        "\n",
        "def make_wakati(sentence):\n",
        "    sentence = tagger.parse(sentence)\n",
        "    sentence = re.sub(r'[0-9０-９a-zA-Zａ-ｚＡ-Ｚ]+', \" \", sentence)\n",
        "    sentence = re.sub(r'[\\．_－―─！＠＃＄％＾＆\\-‐|\\\\＊\\“（）＿■×+α※÷⇒—●★☆〇◎◆▼◇△□(：〜～＋=)／*&^%$#@!~`){}［］…\\[\\]\\\"\\'\\”\\’:;<>?＜＞〔〕〈〉？、。・,\\./『』【】「」→←○《》≪≫\\n\\u3000]+', \"\", sentence)\n",
        "    wakati = sentence.split(\" \")\n",
        "    wakati = list(filter((\"\").__ne__, wakati))\n",
        "    return wakati"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Failed initializing MeCab. Please see the README for possible solutions:\n",
            "\n",
            "    https://github.com/SamuraiT/mecab-python3#common-issues\n",
            "\n",
            "If you are still having trouble, please file an issue here, and include the\n",
            "ERROR DETAILS below:\n",
            "\n",
            "    https://github.com/SamuraiT/mecab-python3/issues\n",
            "\n",
            "issueを英語で書く必要はありません。\n",
            "\n",
            "------------------- ERROR DETAILS ------------------------\n",
            "arguments: -Owakati\n",
            "error message: [ifs] no such file or directory: /usr/local/etc/mecabrc\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-522ace4e31ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtagger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMeCab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"-Owakati\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmake_wakati\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtagger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'[0-9０-９a-zA-Zａ-ｚＡ-Ｚ]+'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/MeCab/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, rawargs)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m             \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTagger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0merror_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrawargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRiqp3dE4TtU"
      },
      "source": [
        "## 単語IDの辞書を定義\n",
        "単語IDの辞書に新しく\\<pad\\>を追加"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PFIZpaKJ4TtV"
      },
      "source": [
        "word2index = {}\n",
        "# 系列を揃えるためのパディング文字列<pad>を追加\n",
        "# パディング文字列のIDは0とする\n",
        "word2index.update({\"<pad>\":0})\n",
        "\n",
        "for title in datasets[\"title\"]:\n",
        "    wakati = make_wakati(title)\n",
        "    for word in wakati:\n",
        "        if word in word2index: continue\n",
        "        word2index[word] = len(word2index)\n",
        "print(\"vocab size : \", len(word2index))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kkf0E5AL4TtW"
      },
      "source": [
        "## 系列の長さを揃えてバッチでまとめる\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "94L9Hwlp4TtX"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import random\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "cat2index = {}\n",
        "for cat in categories:\n",
        "    if cat in cat2index: continue\n",
        "    cat2index[cat] = len(cat2index)\n",
        "    \n",
        "cat2index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9zQ9g3fx4Ttb"
      },
      "source": [
        "def sentence2index(sentence):\n",
        "    wakati = make_wakati(sentence)\n",
        "    return [word2index[w] for w in wakati]\n",
        "\n",
        "def category2index(cat):\n",
        "    return [cat2index[cat]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpA3rR7t4Ttc"
      },
      "source": [
        "index_datasets_title_tmp = []\n",
        "index_datasets_category = []\n",
        "\n",
        "# 系列の長さの最大値を取得。この長さに他の系列の長さをあわせる\n",
        "max_len = 0\n",
        "for title, category in zip(datasets[\"title\"], datasets[\"category\"]):\n",
        "  index_title = sentence2index(title)\n",
        "  index_category = category2index(category)\n",
        "  index_datasets_title_tmp.append(index_title)\n",
        "  index_datasets_category.append(index_category)\n",
        "  if max_len < len(index_title):\n",
        "    max_len = len(index_title)\n",
        "\n",
        "print(type(index_datasets_title_tmp))\n",
        "index_datasets_title_tmp[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "upHP7rGy4Ttf"
      },
      "source": [
        "max_len"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M2cxAQIG4Tth"
      },
      "source": [
        "print(type(index_datasets_category))\n",
        "index_datasets_category[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9AGfLUmD4Ttj"
      },
      "source": [
        "# 系列の長さを揃えるために短い系列にパディングを追加\n",
        "# 後ろパディングだと正しく学習できなかったので、前パディング\n",
        "index_datasets_title = []\n",
        "for title in index_datasets_title_tmp:\n",
        "  for i in range(max_len - len(title)):\n",
        "    title.insert(0, 0) # 前パディング\n",
        "#     title.append(0)　# 後ろパディング\n",
        "  index_datasets_title.append(title)\n",
        "\n",
        "print(type(index_datasets_title))\n",
        "index_datasets_title[:3]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M2IjpMSj4Tts"
      },
      "source": [
        "train_x, test_x, train_y, test_y = train_test_split(index_datasets_title, index_datasets_category, train_size=0.7)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QaV8juDp4Ttt"
      },
      "source": [
        "# データをバッチでまとめるための関数\n",
        "def train2batch(title, category, batch_size=100):\n",
        "  title_batch = []\n",
        "  category_batch = []\n",
        "  title_shuffle, category_shuffle = shuffle(title, category)\n",
        "  for i in range(0, len(title), batch_size):\n",
        "    title_batch.append(title_shuffle[i:i+batch_size])\n",
        "    category_batch.append(category_shuffle[i:i+batch_size])\n",
        "  return title_batch, category_batch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEY64BIX4Ttu"
      },
      "source": [
        "## モデル定義\n",
        "パディング文字列ももちろん埋め込む必要があるわけですが、\\<pad>は0ベクトルで埋め込み、学習の妨げにならないようにする(?)ために、nn.Embedding()にてpadding_idx=0を追加しています。\n",
        "LSTMを定義する際、batch_first=Trueを指定すると、LSTMのインプットの形式がバッチサイズ × 文章の長さ × ベクトル次元数になります。こうしたほうが次元を操作する際にわかりやすいと思います。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJF_dNb14Ttv"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "# GPUを使うために必要\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sllAMZNn4Ttw"
      },
      "source": [
        "test_x = torch.zeros(2, 1, 3)\n",
        "print(test_x.size())\n",
        "test_x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzcXy0Mr4Ttx"
      },
      "source": [
        "test_y = torch.squeeze(test_x)\n",
        "print(test_y.size())\n",
        "test_y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JTuV9iXU4Ttz"
      },
      "source": [
        "class LSTMClassifier(nn.Module):\n",
        "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
        "        super(LSTMClassifier, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        # <pad>の単語IDが0なので、padding_idx=0としている\n",
        "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "        # batch_first=Trueが大事！\n",
        "        # batch_first=Trueを指定すると、LSTMのインプットの形式がバッチサイズ × 文章の長さ(vocab_size) × ベクトル次元数(embedding_dim)になる\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
        "        # LSTMの出力を受け取って全結合してsoftmaxに食わせるための１層のネットワーク\n",
        "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
        "        # softmaxのLog版。dim=0で列、dim=1で行方向を確率変換。\n",
        "        self.softmax = nn.LogSoftmax()\n",
        "\n",
        "    def forward(self, sentence):\n",
        "        embeds = self.word_embeddings(sentence)\n",
        "        # embeds.size() => (batch_size × len(sentence) × embedding_dim)\n",
        "        _, lstm_out = self.lstm(embeds)\n",
        "        # lstm_out[0].size() => (1 × batch_size × hidden_dim)\n",
        "        # lstm_outには2個の要素がある。1つ目が最後の隠れ層の隠れ状態h、2つ目が最後の隠れ層のセル状態c。\n",
        "        # many to one パターンであるため、最後の隠れ層の隠れ状態しか使わないので、lstm_out[0]のみを使う（結合層に渡す）。\n",
        "        tag_space = self.hidden2tag(lstm_out[0])\n",
        "        # tag_space.size() => (1 × batch_size × tagset_size)\n",
        "        # (batch_size × tagset_size)にするためにsqueeze()してから（次元を落としてから）、softmax関数にかける。\n",
        "        # https://pytorch.org/docs/stable/generated/torch.squeeze.html\n",
        "        tag_scores = self.softmax(tag_space.squeeze())\n",
        "        # tag_scores.size() = (batch_size × tagset_size)ｈ\n",
        "\n",
        "        return tag_scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4t-q7464Tt7"
      },
      "source": [
        "# 単語の埋め込み次元数上げた。精度がそこそこアップ！ハイパーパラメータのチューニング大事。\n",
        "EMBEDDING_DIM = 200\n",
        "HIDDEN_DIM = 128\n",
        "VOCAB_SIZE = len(word2index)\n",
        "TAG_SIZE = len(categories)\n",
        "# to(device)でモデルがGPU対応する\n",
        "model = LSTMClassifier(EMBEDDING_DIM, HIDDEN_DIM, VOCAB_SIZE, TAG_SIZE).to(device)\n",
        "loss_function = nn.NLLLoss()\n",
        "# SGDからAdamに変更。特に意味はなし\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Be2Ku6xb4Tt9"
      },
      "source": [
        "## 学習\n",
        "1epoch毎に全バッチを学習させます。バッチごとに逆伝搬してパラメータ更新させてます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2KX5fLV74Tt-"
      },
      "source": [
        "losses = []\n",
        "for epoch in range(100):\n",
        "    all_loss = 0\n",
        "    title_batch, category_batch = train2batch(train_x, train_y)\n",
        "    for i in range(len(title_batch)):\n",
        "        batch_loss = 0\n",
        "\n",
        "        model.zero_grad()\n",
        "\n",
        "        # 順伝搬させるtensorはGPUで処理させるためdevice=にGPUをセット\n",
        "        title_tensor = torch.tensor(title_batch[i], device=device)\n",
        "        # category_tensor.size() = (batch_size × 1)なので、squeeze()\n",
        "        category_tensor = torch.tensor(category_batch[i], device=device).squeeze()\n",
        "\n",
        "        out = model(title_tensor)\n",
        "\n",
        "        batch_loss = loss_function(out, category_tensor)\n",
        "        batch_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        all_loss += batch_loss.item()\n",
        "    losses.append(all_loss)\n",
        "    print(\"epoch\", epoch, \"\\t\" , \"loss\", all_loss)\n",
        "    if all_loss < 0.1: break\n",
        "print(\"done.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AHKhSv0r4TuA"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.plot(losses)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kW1zXDsK4TuB"
      },
      "source": [
        "## 予測\n",
        "バッチ毎にまとめて予測。\n",
        "前回と比べて精度が上がっているのは単語の埋め込み次元数を上げたからだと思われます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2lGNp8Bb4TuC"
      },
      "source": [
        "test_num = len(test_x)\n",
        "a = 0\n",
        "with torch.no_grad():\n",
        "    title_batch, category_batch = train2batch(test_x, test_y)\n",
        "\n",
        "    for i in range(len(title_batch)):\n",
        "        title_tensor = torch.tensor(title_batch[i], device=device)\n",
        "        category_tensor = torch.tensor(category_batch[i], device=device)\n",
        "\n",
        "        out = model(title_tensor)\n",
        "        _, predicts = torch.max(out, 1)\n",
        "        for j, ans in enumerate(category_tensor):\n",
        "            if predicts[j].item() == ans.item():\n",
        "                a += 1\n",
        "print(\"predict : \", a / test_num)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gb8Egc04TuQ"
      },
      "source": [
        "## 最後に\n",
        "バッチ化対応の一番苦労したところはやはり次元の扱いでした。私と同じように次元数によるエラーで躓いている方は、全てのtensorの次元を逐一確認して、データの形状を細かく追ってみるのがよいと思います。"
      ]
    }
  ]
}